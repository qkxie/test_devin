#!/usr/bin/env python3
"""
PageFetcher ä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ PageFetcher ç±»åˆ†æä¸åŒç±»å‹çš„ç½‘ç«™
"""

from page_fetcher import PageFetcher
from loguru import logger
import json
import sys

def analyze_website(fetcher, url, description=""):
    """åˆ†æå•ä¸ªç½‘ç«™å¹¶æ‰“å°ç»“æœ"""
    print(f"\n{'='*60}")
    print(f"åˆ†æç½‘ç«™: {url}")
    if description:
        print(f"æè¿°: {description}")
    print(f"{'='*60}")
    
    try:
        result = fetcher.run(url)
        
        print(f"âœ“ ç½‘ç«™ç±»å‹: {'ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç½‘ç«™' if result['is_chinese'] else 'ğŸŒ éä¸­æ–‡ç½‘ç«™'}")
        print(f"âœ“ ç½®ä¿¡åº¦: {result['confidence']}")
        print(f"âœ“ åˆ¤æ–­ä¾æ®: {result['summary'].get('reason', 'æœªçŸ¥')}")
        
        if result['domain_check']:
            print(f"âœ“ åŸŸåæ£€æŸ¥: {result['domain_check']}")
        
        main_page = result['main_page']
        if main_page:
            print(f"\nğŸ“„ ä¸»é¡µé¢åˆ†æ:")
            print(f"  - ä¸­æ–‡å­—ç¬¦æ•°: {len(main_page.get('chinese_chars', []))}")
            print(f"  - ä¸­å›½åŸå¸‚æ•°: {len(main_page.get('chinese_cities', []))}")
            print(f"  - ä¸­æ–‡å§“æ°æ•°: {len(main_page.get('chinese_surnames', []))}")
            
            if main_page.get('chinese_cities'):
                print(f"  - å‘ç°çš„åŸå¸‚: {', '.join(main_page['chinese_cities'][:5])}")
            if main_page.get('chinese_surnames'):
                print(f"  - å‘ç°çš„å§“æ°: {', '.join(main_page['chinese_surnames'][:5])}")
            
            external_links = main_page.get('external_links', {})
            if any(external_links.values()):
                print(f"  - å¤–é“¾ç»Ÿè®¡:")
                for platform, links in external_links.items():
                    if links:
                        print(f"    * {platform}: {len(links)} ä¸ªé“¾æ¥")
        
        if result['subpages']:
            print(f"\nğŸ“‘ å­é¡µé¢åˆ†æ:")
            for subpage, data in result['subpages'].items():
                print(f"  - {subpage}: å‘ç°ä¸­æ–‡å†…å®¹")
        
        return result
        
    except Exception as e:
        logger.error(f"åˆ†æç½‘ç«™ {url} æ—¶å‡ºé”™: {e}")
        print(f"âŒ é”™è¯¯: {e}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    logger.remove()  # ç§»é™¤é»˜è®¤å¤„ç†å™¨
    logger.add(sys.stderr, level="INFO", format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>")
    logger.add("page_fetcher_example.log", rotation="10 MB", level="DEBUG")
    
    print("ğŸš€ PageFetcher ç½‘é¡µä¸­æ–‡å±æ€§åˆ†æå·¥å…·")
    print("=" * 60)
    
    fetcher = PageFetcher()
    
    test_cases = [
        {
            "url": "https://httpbin.org/html",
            "description": "ç®€å•çš„HTMLæµ‹è¯•é¡µé¢"
        },
        {
            "url": "https://example.com",
            "description": "ç»å…¸çš„ç¤ºä¾‹ç½‘ç«™"
        }
    ]
    
    results = []
    
    for case in test_cases:
        result = analyze_website(fetcher, case["url"], case["description"])
        if result:
            results.append(result)
    
    if results:
        with open("analysis_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ æ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ°: analysis_results.json")
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼å…±åˆ†æäº† {len(results)} ä¸ªç½‘ç«™")

if __name__ == "__main__":
    main()
